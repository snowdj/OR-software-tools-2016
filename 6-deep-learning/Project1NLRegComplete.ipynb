{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning & Neural Networks\n",
    "## Project 1 - Nonlinear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load TensorFlow\n",
    "import tensorflow as tf\n",
    "# Load numpy - adds MATLAB/Julia-style math to Python\n",
    "import numpy as np\n",
    "# Load matplotlib for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# We'll be doing 3D plotting, so we need Axes3D too\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll use the classic Rosenbrock function\n",
    "def rosenbrock(x,y):\n",
    "    a, b = 1.0, 100.0\n",
    "    return (a - x) ** 2 + b * (y - x**2) ** 2\n",
    "# Lets plot our function\n",
    "x = np.outer(np.linspace(-2,+2,100), np.ones(100))\n",
    "y = np.outer(np.ones(100), np.linspace(-1,+4,100))\n",
    "z = rosenbrock(x, y)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x, y, z, cmap=plt.cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets generate some training data\n",
    "N_TRAIN = 500\n",
    "# Sample x from -2 to +2\n",
    "x_train = np.random.rand(N_TRAIN) * 4 - 2\n",
    "# Sample y from -1 to 4\n",
    "y_train = np.random.rand(N_TRAIN) * 5 - 1\n",
    "# Calculate z\n",
    "z_train = rosenbrock(x_train, y_train)\n",
    "# Plot the training set\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x, y, z, cmap=plt.cm.jet, alpha=0.2)\n",
    "ax.scatter(x_train, y_train, z_train, c=\"white\", s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIVE CODING BEGINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OK, lets start with Tensorflow\n",
    "# TF is kind of like JuMP\n",
    "# - Instead of a model, we have a session and a graph\n",
    "# - For \"unknowns\", we have variables in both\n",
    "# - We need to describe relationships between the variables\n",
    "#   and what we're optimizing for too\n",
    "# See slides for architecture\n",
    "# - We'll have N hidden neurons\n",
    "# - Each as three inputs: x, y, and a bias term -> 3N unknowns\n",
    "# - Then the output neuron has N inputs and a bias -> N+1 unknowns\n",
    "\n",
    "N_HIDDEN = 20\n",
    "\n",
    "# Pretty much everything we will be doing is the manipulation of\n",
    "# tensors (or matrices), hence the name.\n",
    "# The first tensor we define is the input to the graph.\n",
    "# When calculating the error of our network, it'd be nice to\n",
    "# calculate the error for multiple training points, or all of\n",
    "# them, simultaneously. The number of points we train on at a\n",
    "# time is the batch size.\n",
    "# So we'll define a placeholder tensor with an unspecified\n",
    "# number of rows and 2 columns (one for x, one for y).\n",
    "net_input = tf.placeholder(tf.float32, [None,2])\n",
    "\n",
    "# We now need to create a tensor for the hidden weights\n",
    "# We have 2 inputs to each unit, and we have N_HIDDEN units,\n",
    "# so we need a 2 x N_HIDDEN weight tensor to calculate the\n",
    "# N_HIDDEN inputs, plus a N_HIDDEN vector of biases\n",
    "\n",
    "# We can initialize the weights and bias to random [-1, +1]\n",
    "hidden_W = tf.Variable(tf.random_uniform([2,N_HIDDEN], -1.0, +1.0))\n",
    "hidden_b = tf.Variable(tf.random_uniform([  N_HIDDEN], -1.0, +1.0))\n",
    "\n",
    "# Now we can create an expression for the combination of the\n",
    "# inputs and this biases. This doesn't actually calculate\n",
    "# anything - its just describing what should be calculated!\n",
    "# Recall that our input_tensor is ? x 2, and hidden_W is 2 x N_H\n",
    "# so the multiplication will be ? x N_H\n",
    "# The bias term is a vector though, so how do we add them?\n",
    "# TF provides a add_bias function that adds the bias to each row\n",
    "hidden_in = tf.nn.bias_add(tf.matmul(net_input, hidden_W), hidden_b)\n",
    "\n",
    "# Now we can put these inputs through our activation function\n",
    "# This is calculated elementwise\n",
    "hidden_out = tf.nn.sigmoid(hidden_in)\n",
    "# tf.nn has other options, check out the documentation!\n",
    "\n",
    "# We now have the output layer. It needs one weight for every\n",
    "# hidden neuron, and a single bias term\n",
    "output_W = tf.Variable(tf.random_uniform([N_HIDDEN,1], -1.0, +1.0))\n",
    "output_b = tf.Variable(tf.random_uniform([1], -1.0, +1.0))\n",
    "output_in = tf.nn.bias_add(tf.matmul(hidden_out, output_W), output_b)\n",
    "net_output = output_in  # linear!\n",
    "\n",
    "# We now need to define our loss function\n",
    "# We'll use good-old square error!\n",
    "# First, create placeholder for the \"true\" value\n",
    "exp_output = tf.placeholder(tf.float32, [None,1])\n",
    "sq_error = tf.square(net_output - exp_output)\n",
    "mse = tf.reduce_mean(sq_error)\n",
    "\n",
    "# We'll use gradient descent to train out network\n",
    "# TensorFlow has many algorithms, and will handle\n",
    "# calculating the derivatives itself!\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.0005) # Sigmoid\n",
    "train_step = optimizer.minimize(mse)\n",
    "\n",
    "# Now, create an operation to initialize all the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Create a Session - Sessions contain all run of a graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initialization operation\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIVE CODING ENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the network to get the output on our training data\n",
    "# Before we do, we need to mash our training data into\n",
    "# a tensor\n",
    "train_xy_tensor = np.vstack((x_train, y_train)).T\n",
    "train_z_tensor = np.reshape(z_train, (N_TRAIN,1))\n",
    "initial_z = sess.run(net_output, feed_dict={\n",
    "                        net_input: train_xy_tensor,\n",
    "                        exp_output: train_z_tensor})\n",
    "initial_z = np.reshape(initial_z, len(initial_z))\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x_train, y_train, z_train, c=\"white\", s=10)\n",
    "ax.scatter(x_train, y_train, initial_z, c=\"purple\", s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run some gradient steps\n",
    "train_errors = []\n",
    "for i in range(10000):\n",
    "    train_error, _ = sess.run((mse, train_step),\n",
    "                            feed_dict={\n",
    "                                net_input: train_xy_tensor,\n",
    "                                exp_output: train_z_tensor})\n",
    "    if i % 500 == 0: print i, train_error\n",
    "    train_errors.append(train_error)\n",
    "plt.plot(range(10000), train_errors, 'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_error, trained_z = sess.run((mse,net_output), feed_dict={\n",
    "                        net_input: train_xy_tensor,\n",
    "                        exp_output: train_z_tensor})\n",
    "trained_z = np.reshape(trained_z, len(trained_z))\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x_train, y_train, z_train, c=\"white\", s=10)\n",
    "ax.scatter(x_train, y_train, trained_z, c=\"purple\", s=10)\n",
    "print z_train[1:10]\n",
    "print trained_z[1:10]\n",
    "print train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we've built our function approximator, we can\n",
    "# look at what it thinks the function is\n",
    "x_test_mat = np.outer(np.linspace(-2,+2,100), np.ones(100))\n",
    "y_test_mat = np.outer(np.ones(100), np.linspace(-1,+4,100))\n",
    "x_test = np.reshape(x_test_mat, (100*100,1))\n",
    "y_test = np.reshape(y_test_mat, (100*100,1))\n",
    "test_xy_tensor = np.hstack((x_test, y_test))\n",
    "net_z_test = sess.run(net_output, feed_dict={net_input: test_xy_tensor})\n",
    "z_test_mat = np.reshape(net_z_test, (100,100))\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x_test_mat, y_test_mat, z_test_mat, cmap=plt.cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.outer(np.linspace(-2,+2,100), np.ones(100))\n",
    "y = np.outer(np.ones(100), np.linspace(-1,+4,100))\n",
    "z = rosenbrock(x, y)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x, y, z, cmap=plt.cm.jet)\n",
    "print np.mean((z - z_test_mat)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
